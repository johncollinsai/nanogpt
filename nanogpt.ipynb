{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75874666-e3ac-4cc7-ac24-b38a97ef5f8a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-05T21:36:41.372727Z",
     "iopub.status.busy": "2023-02-05T21:36:41.372173Z",
     "iopub.status.idle": "2023-02-05T21:36:41.379077Z",
     "shell.execute_reply": "2023-02-05T21:36:41.377483Z",
     "shell.execute_reply.started": "2023-02-05T21:36:41.372688Z"
    }
   },
   "source": [
    "# This post in a glance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "386c1731-7a29-43a4-8baf-71c9db62a787",
   "metadata": {},
   "source": [
    "Large Language Models (LLMs) have created enormous interest. Here I build and train a Generatively Pretrained Transformer (GPT) following __[Kaparthy's approah](https://www.youtube.com/watch?v=kCc8FmEb1nY)__ in order to understand the under-the-hood components of what makes a GPT work and obtain insights into how GPTs can be applied in real-life commercial applications. This post is a hands on exploration of the transformer architecture first set out in the seminal 2017 AI paper __[Attention Is All You Need](https://arxiv.org/abs/1706.03762)__, arguably one of the most important AI papers written, and the 2020 GPT-3 paper __[Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)__. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6fb20065",
   "metadata": {},
   "source": [
    "When we move from neural networks to transformers specifically, attention, or more specficially self-attention, is the super-important thing.  So what is attention, or self-attention, exactly?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3c1d218c",
   "metadata": {},
   "source": [
    "**Attention**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26a4996",
   "metadata": {},
   "source": [
    "Both attention and self-attention are mechanisms for processing variable-length inputs, such as natural language sentences or images.  Attention computes a weighted sum of a set of values, based on the similarity between a query and a set of keys. The keys and values can be different from each other, and are often used to represent different parts of the input sequence or different features of the data. The query is typically derived from the current hidden state of the model, and is used to focus the attention on a specific part of the input. The resulting weighted sum is a fixed-size representation of the input that captures the relevant information for the current task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5673a1c4",
   "metadata": {},
   "source": [
    "**Self-attention**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0feef914",
   "metadata": {},
   "source": [
    "Self-attention, on the other hand, is a specific form of attention where the query, key, and value vectors are all derived from the same set of input vectors. In other words, self-attention computes a weighted sum of the input vectors themselves, rather than a set of separate values. Self-attention computes a context-aware representation of each word based on its relationship to the other words in the sequence and can thus be used to capture long-range dependencies in the input sequence, such as when a word at the beginning of the sentence is related to a word at the end of the sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e70c351",
   "metadata": {},
   "source": [
    "**Expressed mathematically**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "966dd824",
   "metadata": {},
   "source": [
    "Mathematically, self-attention may be written:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1829638a",
   "metadata": {},
   "source": [
    "$$A = \\text{softmax} \\left( \\frac{QK^T}{\\sqrt{d_k}} \\right) V$$ "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3868693d",
   "metadata": {},
   "source": [
    "where $Q$, $K$, and $V$ are matrices representing the queries, keys, and values in the attention mechanism, and $d_k$ is the dimension of the key vectors. The softmax function is applied element-wise to the matrix resulting from the dot product of $Q$ and the transpose of $K$ divided by the square root of $d_k$. The resulting matrix is then multiplied by $V$ to obtain the final output of the attention mechanism."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "23ec0a89",
   "metadata": {},
   "source": [
    "**Set up for this post**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3e305541",
   "metadata": {},
   "source": [
    "I begin with an empty file and define a transformer piece by piece.  Then I train it on a text dataset.  I will accelarate some of the operations using the GPU on my machine and I will build a deep learning framework in a virtual environment for this project that includes PyTorch, TensorFlow, CUDA, cuDNN, and NVIDIA Drivers, on Ubuntu 22.04 LTS.  Source code for this post may be found on __[my GitHub](https://github.com/johncollinsai/nanogpt)__."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fed8f79-c4d8-47e5-b04f-d60e0bb25101",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550c7378-6a41-49ff-880b-a7892a39f5cd",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Insights and takeaways:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf034c08-7827-4e5d-a2b8-39b4ed78012f",
   "metadata": {},
   "source": [
    "> * A GPT is a large language model.  Essentially, it streams a set of predictions that are the most likely tokens (words) in response to your question, which is an input to the language model in tokens (or words).  There is nothing magical about a GPT, nor any other LLM; in response to your input they provide an optimized prediction or outcome. <br>\n",
    "> * Any magic that might exist is the magic bestowed by \"attention\", more specifically, self-attention. <br>\n",
    "> * Self-attention is a mechanism for processing variable-length inputs, such as natural language sentences or images. Self-attention is a way for a model to weigh the importance of different parts of its inputs when making predictions, based on their relevance to the task at hand. \n",
    "> * Insights <br>\n",
    "> * Insights <br>\n",
    "> * Insights <br>\n",
    "> * Insights <br>\n",
    "> * [See Karpathy's conclusions (video 01:54:32) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af203336-ddb2-4de8-a9cb-6b7cdb690bac",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896def4a-31c8-4aa2-805e-4ecd5231a29c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-05T21:37:21.573542Z",
     "iopub.status.busy": "2023-02-05T21:37:21.572887Z",
     "iopub.status.idle": "2023-02-05T21:37:21.577911Z",
     "shell.execute_reply": "2023-02-05T21:37:21.576606Z",
     "shell.execute_reply.started": "2023-02-05T21:37:21.573523Z"
    }
   },
   "source": [
    "# Building and training a GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b9b1fc-b610-44c1-885d-1935aee8207e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Baseline (bigram) language model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bfd5cb",
   "metadata": {},
   "source": [
    "#### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4857982f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "Device name: NVIDIA GeForce RTX 3080 Ti Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "# check GPU\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"device: {device}\")\n",
    "    print(f\"Device name: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "486b06cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable use of GPU following Karpathy's method, see video ~39:00 and https://github.com/karpathy/ng-video-lecture/blob/master/bigram.py\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "416923fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataseet in characters:  1115394\n"
     ]
    }
   ],
   "source": [
    "# get data\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(\"length of dataseet in characters: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81a44601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "# show unique characters appearing in the dataset (note the space character, which is first in the set): i.e., the vocabulary of possible characters the model can see or emit\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c21805",
   "metadata": {},
   "source": [
    "#### Tokenize\n",
    "\n",
    "> Build a simple encoder and  decoder: i.e., take a string, output a list of integers, where each character is a token. The approach below is similar to, but much more simplified than: __[goolge sentencepiece](https://github.com/google/sentencepiece)__ (which uses sub-word encodings) and __[OpenAI tiktoken](https://github.com/openai/tiktoken)__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17bc426f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[58, 53, 42, 39, 63, 1, 47, 57, 1, 44, 56, 47, 42, 39, 63, 6, 1, 50, 53, 53, 49, 47, 52, 45, 1, 44, 53, 56, 61, 39, 56, 42, 1, 58, 53, 1, 58, 46, 43, 1, 61, 43, 43, 49, 43, 52, 42, 2]\n",
      "today is friday, looking forward to the weekend!\n"
     ]
    }
   ],
   "source": [
    "# convert the raw text as a string into some sequence of integers according to some vocabulary of possible elements\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "# build a simple encoder and decoder, effectively a tokenizer and detokenizer\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "print(encode(\"today is friday, looking forward to the weekend!\"))\n",
    "print(decode(encode(\"today is friday, looking forward to the weekend!\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc914948",
   "metadata": {},
   "source": [
    "> now I have a tokenizer and detokenizer, I can convert the raw text into a sequence of integers, i.e., I can tokenize the entire training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2edc8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
     ]
    }
   ],
   "source": [
    "# encode training dataset and store it in a torch.tensor\n",
    "import torch\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c31f70-00ec-4a57-b6b9-c610454e7f39",
   "metadata": {},
   "source": [
    "#### Train/val split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff404893-dff5-4e84-9e12-92f11cfc34b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 90:10 train:val split\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8320008e-4209-4004-b6ed-6c2cc9b3eb4c",
   "metadata": {},
   "source": [
    "#### Loading the data\n",
    "> I set the time dimension (i.e., the contexts) of the tensors feeding into the transformer equal to a maximum of 8 characters (i.e., I set block_size = 8).  Note: I train on block_size+1 because the transformer trains on the first 8 characters and predicts the +1th or 9th character.  Put another way, the transformer sees contexts from one character thru block_size. <br>\n",
    "\n",
    "> And I set the batch dimension of the tensors feeding into the transformer to 4, so batch_size = 4 (i.e., 4 independent sequences will be processed in parallel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26380bac-baa0-47a7-b19a-536b6b0f1caf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set block_size = 8 to train on []:block_size+1] = 8+1 characters at a time\n",
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ebfdae44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Illustrating how the transformer trains on the first 8 characters and predicts the +1th or 9th character:\n",
      "when input is tensor([18]), the target is 47\n",
      "when input is tensor([18, 47]), the target is 56\n",
      "when input is tensor([18, 47, 56]), the target is 57\n",
      "when input is tensor([18, 47, 56, 57]), the target is 58\n",
      "when input is tensor([18, 47, 56, 57, 58]), the target is 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]), the target is 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]), the target is 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]), the target is 58\n"
     ]
    }
   ],
   "source": [
    "# +1 because we want to predict the next character, thus block_size+1 allows us to do that, i.e., the transformer trains on the first 8 characters and predicts the +1th or 9th character\n",
    "# to illustrate:\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "print('Illustrating how the transformer trains on the first 8 characters and predicts the +1th or 9th character:')\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f'when input is {context}, the target is {target}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca53410",
   "metadata": {},
   "source": [
    "#### A quick note on random seed selection\n",
    "\n",
    "> In an interesting __[paper](https://arxiv.org/abs/2109.08203)__ David Picard investigates the effect of random seed selection on accuracy when using deep learning architectures for computer vision and posits that Torch.manual_seed(3407) is all you need!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1fb66009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the tensor input to the transformer: \n",
      " tensor([[32, 39, 49, 43,  1, 58, 46, 53],\n",
      "        [59, 56,  1, 54, 56, 47, 52, 41],\n",
      "        [57, 53, 51, 43,  1, 51, 43, 56],\n",
      "        [57,  1, 58, 56, 59, 43,  2,  0]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# I set the batch dimension of the tensors feeding into the transformer to 4, so batch_size = 4 (i.e., 4 independent sequences will be processed in parallel).\n",
    "torch.manual_seed(3407)\n",
    "batch_size = 4\n",
    "block_size = 8  \n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,)) \n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device) # move data to GPU\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "\n",
    "print('Here is the tensor input to the transformer:',\n",
    "      '\\n', \n",
    "      xb      \n",
    "      )  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625a9b6b-3710-4454-a648-2daef588b99f",
   "metadata": {},
   "source": [
    "#### baseline (bigram) language model\n",
    "\n",
    "Following __[Karpathy](https://www.youtube.com/watch?v=kCc8FmEb1nY)__, I implement a very simple neural network, the bigram language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6bec0de5-afc3-4ada-8c24-ef2e5325d7e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits shape: torch.Size([256, 65])\n",
      "loss: tensor(4.5789, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "\n",
      "pTFwSpp,f.v-;LR-;DA,O:rGMbv3OqDlpuo-SxIMtqCPawLaD;iC O'-N$sr?,y;Dgx&uJvha?qU.RXFqe!3CLnq,ZAcdW-dxvq\n",
      "ijb-dmxN-lLtI'UsNajeE3gH??!m3zz:nMgrVgHyRJd;MVWy'nEDSCT!QA;myMPVPLnvyjMWXFw,LweP,WSzdPrvcWXecNIcLtcPrPbGIzVH.nqckUK;XfAco',QFJ3'T !a-$Nemy,WmkUIx?mO!sJwEywCCk,W:Jv3V&PjhvEooQF3taT\n",
      "3&u!XCikXcY\n",
      "?xIzQrGW\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(3407)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx and targets are both (B, T) tensors of integers\n",
    "        logits = self.token_embedding_table(idx) # (B, T, C, i.e., a batch by time (context) by channel tensor, where channel is vocab size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # reorganize logits tensor from (B, T, C) to (B*T, C) in order to fit pytorch's cross_entropy loss function\n",
    "            B, T, C = logits.shape \n",
    "            logits = logits.view(B*T, C) \n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets) # cross_entropy here computes negative log likelihood loss\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is a (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get the probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat([idx, idx_next], dim=1) # (B, T+1)\n",
    "\n",
    "        return idx\n",
    "    \n",
    "model = BigramLanguageModel(vocab_size)\n",
    "m = model.to(device) # move model to GPU\n",
    "logits, loss = m(xb, yb)\n",
    "print('logits shape:', logits.shape)\n",
    "print('loss:', loss)\n",
    "\n",
    "# context = torch.zeros((1,1), dtype=torch.long, device=device), here created on-the-fly by print() on the GPU\n",
    "print(decode(m.generate(idx = torch.zeros((1,1), dtype=torch.long, device=device), max_new_tokens=300)[0].tolist())) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2830757",
   "metadata": {},
   "source": [
    "> The model is untrained and provides predictions that are random, so the output is meaningless."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a3b698-2682-4dfc-9ff1-8a420d51e037",
   "metadata": {},
   "source": [
    "#### Training the bigram model\n",
    "\n",
    "> I now train the bigram model to make it less random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c539ac77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pytorch optimizer\n",
    "optimizer = torch.optim.Adam(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "df0b42ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.5604467391967773\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32 # increase the batch size from 4 to 32 to speed up training\n",
    "for steps in range(10000): # increase the number of steps to train for, to improve results\n",
    "\n",
    "    # get a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print('loss:', loss.item()) # training for 10000 steps brings the loss down to ~2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2c16952a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Afre an nthe t y.\n",
      "I twl d, h bstave: anr h? towll\n",
      "BUStil ilouniasthechyord IF shaty vouby, m aysie fon Malld aty acoghas; histhofok ang titr. may, o mar, we gel adadico y mereengoowe.\n",
      "Hiplouplloproousseathes l we f Jater, thee,\n",
      "\n",
      "Hwncoshy momyow, r agh afurst thes hendee: byoon t MIILELIZMend, cuthe\n"
     ]
    }
   ],
   "source": [
    "# As above, context = torch.zeros((1,1), dtype=torch.long, device=device), here created on-the-fly by print() on the GPU\n",
    "print(decode(m.generate(idx = torch.zeros((1,1), dtype=torch.long, device=device), max_new_tokens=300)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12526d05",
   "metadata": {},
   "source": [
    "> The model is making progress.  But it's still a very simple model and the tokens are not yet talking to each other.  It's predictions show a somewhat better language-like structure, but are still random, and the output meaningless."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b03421a",
   "metadata": {},
   "source": [
    "***\n",
    "#### bigram.py\n",
    "\n",
    "> At ~38:00 in the video Karpathy shows bigram.py, which is available in the __[ng-video-lecture](https://github.com/karpathy/ng-video-lecture)__ repo. <br>\n",
    "\n",
    "\n",
    "> **HOWEVER, IT IS NOT CLEAR AT THIS POINT IF bigram.py IS NEEDED, SO I AM SKIPPING IT FOR THE MOMENT** <br>\n",
    "\n",
    "***\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "46d1a718-bb5f-4479-a576-465adde0082c",
   "metadata": {},
   "source": [
    "## Self-attention\n",
    "\n",
    "> I now write the first self-attention block for processing the tokens, following several steps, each progressively more effective, that hopefully help to make the self-attention contstruct clearer. <br>\n",
    "\n",
    "> Let's start with a very simple example, which essentially relates tokens to each other via their history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7b3efd10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# simple example\n",
    "torch.manual_seed(3407)\n",
    "B,T,C = 4,8,2 # batch size, time steps, channels\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "65089f35-33e4-45de-821c-e56ac1f5863e",
   "metadata": {},
   "source": [
    "#### Averaging past context with for loops (weakest form of aggregation)\n",
    "\n",
    "> A simple way to enable tokens to communicate in the manner we desire (i.e., with the tokens that precede them in T), is to calculate an average of all the preceding elements. Consider, for example, the fifth token: take the channels that make up that information at that step, but also the channels from the fourth step, third step, second and first steps, and average them.  This creates, effectively, a feature vector that summarizes the 5th token in the context of its history.  An average like this is an extremely weak and lossy, i.e., a lot of information about the spacial arrangements of the tokens is lost. <br>\n",
    "\n",
    "> So, for every batch element independently, for every $n^{th}$ token in that sequence, calculate the average of all the vectors in all the previous tokens and also at the $n^{th}$ token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cb497036-c809-44fc-8152-331cb6594706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1703, -0.8613],\n",
      "        [-0.6225,  1.0247],\n",
      "        [ 0.3506,  0.8032],\n",
      "        [ 0.0865, -0.9623],\n",
      "        [-1.6784,  1.3681],\n",
      "        [-0.1882,  1.7510],\n",
      "        [ 0.5818, -0.3983],\n",
      "        [ 1.4324, -0.6142]])\n",
      "xbow averages everything up to the current location of the nth token:  \n",
      " tensor([[ 0.1703, -0.8613],\n",
      "        [-0.2261,  0.0817],\n",
      "        [-0.0339,  0.3222],\n",
      "        [-0.0038,  0.0011],\n",
      "        [-0.3387,  0.2745],\n",
      "        [-0.3136,  0.5206],\n",
      "        [-0.1857,  0.3893],\n",
      "        [ 0.0166,  0.2639]])\n"
     ]
    }
   ],
   "source": [
    "# I want x[b,t] = mean_{i<=t} x[b,i]\n",
    "xbow = torch.zeros((B,T,C)) # bow for bag of words\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b,:t+1] # (t, C)\n",
    "        xbow[b,t] = torch.mean(xprev, 0)\n",
    "\n",
    "print(x[0])\n",
    "print('xbow averages everything up to the current location of the nth token: ', '\\n',\n",
    "      xbow[0])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0e8b09bd-e7b5-4de1-985f-8423193eb8ac",
   "metadata": {},
   "source": [
    "#### Self-attention: matrix multiply as weighted aggregation\n",
    "\n",
    "> Karpathy shows how to use matrix multiplication to increase the efficiency of the above operation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e9ba9f42-1d4b-4aba-bc74-8d8a58f69272",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei = torch.tril(torch.ones((T,T))) # wei denotes weights, torch.tril provides lower triangular matrix\n",
    "wei = wei / wei.sum(1, keepdim=True) # normalize weights so that they sum to 1\n",
    "xbow2 = wei @ x #  (B, T, T) @ (B, T, C) --> (B, T, C)\n",
    "torch.allclose(xbow, xbow2) # check that the two methods give the same result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "21b513a4-5e38-46b7-9d4c-eb7fd43ce2d9",
   "metadata": {},
   "source": [
    "#### Adding softmax\n",
    "\n",
    "> Applying a softmax to each row to normalize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8783fa4f-c174-4a88-80cb-be8738e08ae1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril = torch.tril(torch.ones((T,T))) # tril matrix of lower triangular ones\n",
    "wei = torch.zeros((T,T)) # wei begins as a matrix of zeros\n",
    "wei = wei.masked_fill(tril == 0, float('-inf')) # weights for the future tokens are set to -inf, so future tokens are ignored\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "xbow3 = wei @ x\n",
    "torch.allclose(xbow, xbow3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f52a3b2-cff8-484f-bc62-4489d7b678cc",
   "metadata": {},
   "source": [
    "#### Positional encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169bd6e6-1548-405e-8aaa-a7d8658e6423",
   "metadata": {},
   "source": [
    "[Details]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "af8a01da-694c-4159-85ca-b49135fd23ef",
   "metadata": {},
   "source": [
    "#### Self-attention\n",
    "\n",
    "> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b6f54128-e5c2-467e-bd81-2a40327dc60a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "torch.manual_seed(3407)\n",
    "B, T, C = 4, 8, 32 # batch size, time steps, channels\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "# Observe a single Head perform self-attention\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x) # (B, T, head_size)\n",
    "q = query(x) # (B, T, head_size)\n",
    "wei = q @ k.transpose(-2,-1) # (B, T, head_size) @ (B, head_size, T) --> (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones((T,T))) \n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "v = value(x) \n",
    "out = wei @ v \n",
    "\n",
    "out.shape # (B, T, head_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e822fd86",
   "metadata": {},
   "source": [
    "> Observe the weights, as a matrix of lower triangular values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a87d0047",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.9217, 0.0783, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2666, 0.1544, 0.5789, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0332, 0.4348, 0.2287, 0.3034, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0853, 0.0492, 0.1398, 0.5914, 0.1343, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0553, 0.2033, 0.0449, 0.5934, 0.0320, 0.0711, 0.0000, 0.0000],\n",
       "        [0.1148, 0.0771, 0.0900, 0.0522, 0.0507, 0.2886, 0.3266, 0.0000],\n",
       "        [0.1356, 0.0336, 0.0196, 0.0464, 0.0245, 0.2620, 0.2610, 0.2173]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc35e5cc-a02e-40da-8f57-ee09ffa4c267",
   "metadata": {},
   "source": [
    "Notes\n",
    "\n",
    "> * Attention as communication <br>\n",
    "\n",
    "> * Attention has no notion of space, operates over sets\n",
    "\n",
    "> * There is no communication across batch dimension\n",
    "\n",
    "> * Encoder blocks vs. decoder blocks\n",
    "\n",
    "> * Attention vs. self-attention vs. cross-attention\n",
    "\n",
    "> * \"Scaled\" self-attention.  Why divide by sqrt(head_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f818f9-4516-4c3f-9b8f-2d8de4beb9b1",
   "metadata": {},
   "source": [
    "## Building the Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f07d79-0203-4558-93a0-5e8974b317ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-09T04:03:37.171153Z",
     "iopub.status.busy": "2023-02-09T04:03:37.170812Z",
     "iopub.status.idle": "2023-02-09T04:03:37.176160Z",
     "shell.execute_reply": "2023-02-09T04:03:37.174873Z",
     "shell.execute_reply.started": "2023-02-09T04:03:37.171132Z"
    }
   },
   "source": [
    "#### Inserting a single self-attention block "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6081db1b-3530-4863-aff1-ee0d33f7f24f",
   "metadata": {},
   "source": [
    "[Details]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4111fee3-add9-4c60-926d-b02395b6666a",
   "metadata": {},
   "source": [
    "#### Multi-headed self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e56e79-4c09-470e-bf40-b6afc39ff475",
   "metadata": {},
   "source": [
    "[Details]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6addebf6-d82d-49b8-b803-8b4a1490459a",
   "metadata": {},
   "source": [
    "#### feedforward layers of transformer block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f920a6-8c7b-454c-b586-e36560648f43",
   "metadata": {},
   "source": [
    "[Details]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef32abc-c9fb-444e-9156-05bb5f9e1435",
   "metadata": {},
   "source": [
    "#### Residual connections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd755d2-bfbf-4af9-8852-5b5937560135",
   "metadata": {},
   "source": [
    "[Details]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bf309b-a584-43c1-8ce7-49262dc145ee",
   "metadata": {},
   "source": [
    "#### Layernorms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ecd508-8acb-45e8-8d8b-9a65b9ebf3b3",
   "metadata": {},
   "source": [
    "[Details]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d7d21b-cf7e-4ce7-ad6b-2b4e7d2f666c",
   "metadata": {},
   "source": [
    "#### Scaling up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1defac21-b22f-40d7-9cc0-b534f6360e50",
   "metadata": {},
   "source": [
    "[Details]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c338caa-0589-49a0-9ace-b179fa6acb88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-05T07:25:07.934342Z",
     "iopub.status.busy": "2023-02-05T07:25:07.934053Z",
     "iopub.status.idle": "2023-02-05T07:25:07.940165Z",
     "shell.execute_reply": "2023-02-05T07:25:07.938862Z",
     "shell.execute_reply.started": "2023-02-05T07:25:07.934322Z"
    }
   },
   "source": [
    "## [Notes on Transformer]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b675729-095d-40c2-9ba9-43f5794c17f3",
   "metadata": {},
   "source": [
    "#### Encoder vs. decoder vs. both"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3be788-107e-4786-8cde-1f9acef2007a",
   "metadata": {},
   "source": [
    "#### Batched multi-headed self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f59f7d-55d5-4525-9e94-5f77e37db10b",
   "metadata": {},
   "source": [
    "#### ChatGPT, GPT-3, pretraining vs. finetuning, RLHF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2ef3c3",
   "metadata": {},
   "source": [
    "## Summary diagrams for the transformer architecture\n",
    "\n",
    "> The following diagrams can be found at __[Anton Bacaj's github](https://github.com/abacaj/transformers)__.  They are great summaries of the transformer architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a85d42",
   "metadata": {},
   "source": [
    "#### Decoder models\n",
    "<img src=\"decoder-formatted.png\" alt=\"decoder models\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eae9b0a",
   "metadata": {},
   "source": [
    "#### Encoder models\n",
    "<img src=\"encoder-formatted.png\" alt=\"decoder models\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5756d48a",
   "metadata": {},
   "source": [
    "#### Encoder + decoder models\n",
    "<img src=\"enc+dec-formatted.png\" alt=\"decoder models\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea89cde2-ec75-4b03-8ea5-2542d4c8427d",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efee8c82-06f7-4869-8671-eb941167b3b4",
   "metadata": {},
   "source": [
    "Brown, T.B., et al. (2020). Language Models are Few-Shot Learners. arXiv:2005.14165"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc668cf-bc7f-4d43-a6de-690a28cb9897",
   "metadata": {},
   "source": [
    "__[Colab for Kaparthy's video](https://colab.research.google.com/drive/1JMLa53HDuA-i7ZBmqV7ZnA3c_fvtXnx-?usp=sharing)__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e95107",
   "metadata": {},
   "source": [
    "__[GitHub repo for Anton Bacaj's transformer architecture diagrams](https://github.com/abacaj/transformers)__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7673317d-5bc1-4817-b349-1900a9410cc2",
   "metadata": {},
   "source": [
    "__[GitHub repo for Kaparthy's video](https://github.com/karpathy/ng-video-lecture)__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad7cab4-1542-4340-80ad-1989bcf689a2",
   "metadata": {},
   "source": [
    "__[Kaparthy's nanoGPT GitHub repo](https://github.com/karpathy/nanoGPT)__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f481fea7-f09f-409f-99fd-67c29b6a788c",
   "metadata": {},
   "source": [
    "__[Kaparthy's Youtube video](https://www.youtube.com/watch?v=kCc8FmEb1nY)__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7333d09d",
   "metadata": {},
   "source": [
    "Picard, D. (2021). Torch.manual_seed(3407) is all you need: On the influence of random seeds in deep learning architectures for computer vision. arXiv:2109.08203 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99032892-1126-456b-8baf-424b0b00552a",
   "metadata": {},
   "source": [
    "Vaswani, A., et al. (2017).  Attention Is All You Need. arXiv:1706.03762"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04df5bcd-28f1-4d99-b7f8-f6685edc947c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-09T04:06:52.202330Z",
     "iopub.status.busy": "2023-02-09T04:06:52.202069Z",
     "iopub.status.idle": "2023-02-09T04:06:52.355032Z",
     "shell.execute_reply": "2023-02-09T04:06:52.354010Z",
     "shell.execute_reply.started": "2023-02-09T04:06:52.202312Z"
    }
   },
   "source": [
    "***\n",
    "END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
