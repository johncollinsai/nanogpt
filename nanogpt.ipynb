{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75874666-e3ac-4cc7-ac24-b38a97ef5f8a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-05T21:36:41.372727Z",
     "iopub.status.busy": "2023-02-05T21:36:41.372173Z",
     "iopub.status.idle": "2023-02-05T21:36:41.379077Z",
     "shell.execute_reply": "2023-02-05T21:36:41.377483Z",
     "shell.execute_reply.started": "2023-02-05T21:36:41.372688Z"
    }
   },
   "source": [
    "# This post in a glance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386c1731-7a29-43a4-8baf-71c9db62a787",
   "metadata": {},
   "source": [
    "Large Language Models (LLMs) have created enormous interest. Here I build and train a Generatively Pretrained Transformer (GPT) following __[Kaparthy's approah](https://www.youtube.com/watch?v=kCc8FmEb1nY)__ in order to understand the under-the-hood components of what makes a GPT work and obtain insights into how GPTs can be applied in real-life commercial applications. <br>\n",
    "\n",
    "This post is a hands on exploration of the transformer architecture first set out in the seminal 2017 AI paper __[Attention Is All You Need](https://arxiv.org/abs/1706.03762)__, arguably the most important AI paper ever written, and the 2020 GPT-3 paper __[Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)__."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fed8f79-c4d8-47e5-b04f-d60e0bb25101",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550c7378-6a41-49ff-880b-a7892a39f5cd",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Insights:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf034c08-7827-4e5d-a2b8-39b4ed78012f",
   "metadata": {},
   "source": [
    "> * DESCRIBE IN ONE SENTENCE WHAT ATTENTION IS AND WHY, THEREFORE, TRANSFORMERS ARE SUCH A SIGNIFICANT STEP IN THE EVOLUTION OF AI <br>\n",
    "> * DESCRIBE IN ONE SENTENCE WHAT A GPT IS AND WHY IT, TOO, IS AN IMPORTANT STEP ON THE EVOLUTION OF AI <br>\n",
    "> * SO WHAT? EXPLAIN IN ONE SENTENCE HOW GPTs SHALL TRANSFORM BUSINESS <br>\n",
    "> * HOW? PROVIDE AN EXAMPLE OF HOW GPTs SHALL TRANSFORM BUSINESS <br>\n",
    "> * Insights <br>\n",
    "> * Insights <br>\n",
    "> * Insights <br>\n",
    "> * Insights <br>\n",
    "> * Insights <br>\n",
    "> * [See Karpathy's conclusions (video 01:54:32) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caff1406-ef8d-4e31-bfc9-33238b211b52",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b5996e-9a77-48a1-9e4f-802711b78965",
   "metadata": {},
   "source": [
    "#### The rest of this post ... [DOES WHAT?]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af203336-ddb2-4de8-a9cb-6b7cdb690bac",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896def4a-31c8-4aa2-805e-4ecd5231a29c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-05T21:37:21.573542Z",
     "iopub.status.busy": "2023-02-05T21:37:21.572887Z",
     "iopub.status.idle": "2023-02-05T21:37:21.577911Z",
     "shell.execute_reply": "2023-02-05T21:37:21.576606Z",
     "shell.execute_reply.started": "2023-02-05T21:37:21.573523Z"
    }
   },
   "source": [
    "# Building and training a GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b9b1fc-b610-44c1-885d-1935aee8207e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Baseline (bigram) language model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24412df2-d41e-4ff2-80dc-90cf56d28f01",
   "metadata": {
    "tags": []
   },
   "source": [
    "[Intro]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a283f1c5-0ab2-4fc7-9a08-b7d76a8b55d1",
   "metadata": {},
   "source": [
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1199a73-9e02-4635-9e4c-db741b22ce8a",
   "metadata": {},
   "source": [
    "[Details]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c31f70-00ec-4a57-b6b9-c610454e7f39",
   "metadata": {},
   "source": [
    "#### Train/val split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff404893-dff5-4e84-9e12-92f11cfc34b5",
   "metadata": {},
   "source": [
    "[Details]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8320008e-4209-4004-b6ed-6c2cc9b3eb4c",
   "metadata": {},
   "source": [
    "#### Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26380bac-baa0-47a7-b19a-536b6b0f1caf",
   "metadata": {},
   "source": [
    "[Details]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625a9b6b-3710-4454-a648-2daef588b99f",
   "metadata": {},
   "source": [
    "#### baseline (bigram) language model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bec0de5-afc3-4ada-8c24-ef2e5325d7e2",
   "metadata": {},
   "source": [
    "[Details]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a3b698-2682-4dfc-9ff1-8a420d51e037",
   "metadata": {},
   "source": [
    "#### Training the bigram model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a589398b-7567-4c24-91b0-5c32d255be16",
   "metadata": {},
   "source": [
    "[Details]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a213751c-62eb-4172-8ead-64dfb75779b2",
   "metadata": {},
   "source": [
    "#### Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34484a3-ecf5-4381-8f52-0d7b7fee0498",
   "metadata": {},
   "source": [
    "[Details]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d1a718-bb5f-4479-a576-465adde0082c",
   "metadata": {},
   "source": [
    "## Self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65089f35-33e4-45de-821c-e56ac1f5863e",
   "metadata": {},
   "source": [
    "#### Averaging past context with for loops (weakest form of aggregation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb497036-c809-44fc-8152-331cb6594706",
   "metadata": {},
   "source": [
    "[Details]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8b09bd-e7b5-4de1-985f-8423193eb8ac",
   "metadata": {},
   "source": [
    "#### Self-attention: matrix multiply as weighted aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305043fc-1380-4b25-94ee-a16fe0950ee7",
   "metadata": {},
   "source": [
    "[Details]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0b3945-9457-4b21-9107-800c43e3486b",
   "metadata": {},
   "source": [
    "#### Using matrix multiply"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ba9f42-1d4b-4aba-bc74-8d8a58f69272",
   "metadata": {},
   "source": [
    "[Details]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b513a4-5e38-46b7-9d4c-eb7fd43ce2d9",
   "metadata": {},
   "source": [
    "#### Adding softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8783fa4f-c174-4a88-80cb-be8738e08ae1",
   "metadata": {},
   "source": [
    "[Details]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f52a3b2-cff8-484f-bc62-4489d7b678cc",
   "metadata": {},
   "source": [
    "#### Positional encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169bd6e6-1548-405e-8aaa-a7d8658e6423",
   "metadata": {},
   "source": [
    "[Details]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8a01da-694c-4159-85ca-b49135fd23ef",
   "metadata": {},
   "source": [
    "#### Self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f54128-e5c2-467e-bd81-2a40327dc60a",
   "metadata": {},
   "source": [
    "[Details]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc35e5cc-a02e-40da-8f57-ee09ffa4c267",
   "metadata": {},
   "source": [
    "Notes\n",
    "\n",
    "> * Attention as communication <br>\n",
    "\n",
    "> * Attention has no notion of space, operates over sets\n",
    "\n",
    "> * There is no communication across batch dimension\n",
    "\n",
    "> * Encoder blocks vs. decoder blocks\n",
    "\n",
    "> * Attention vs. self-attention vs. cross-attention\n",
    "\n",
    "> * \"Scaled\" self-attention.  Why divide by sqrt(head_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f818f9-4516-4c3f-9b8f-2d8de4beb9b1",
   "metadata": {},
   "source": [
    "## Building the Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f07d79-0203-4558-93a0-5e8974b317ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-09T04:03:37.171153Z",
     "iopub.status.busy": "2023-02-09T04:03:37.170812Z",
     "iopub.status.idle": "2023-02-09T04:03:37.176160Z",
     "shell.execute_reply": "2023-02-09T04:03:37.174873Z",
     "shell.execute_reply.started": "2023-02-09T04:03:37.171132Z"
    }
   },
   "source": [
    "#### Inserting a single self-attention block "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6081db1b-3530-4863-aff1-ee0d33f7f24f",
   "metadata": {},
   "source": [
    "[Details]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4111fee3-add9-4c60-926d-b02395b6666a",
   "metadata": {},
   "source": [
    "#### Multi-headed self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e56e79-4c09-470e-bf40-b6afc39ff475",
   "metadata": {},
   "source": [
    "[Details]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6addebf6-d82d-49b8-b803-8b4a1490459a",
   "metadata": {},
   "source": [
    "#### feedforward layers of transformer block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f920a6-8c7b-454c-b586-e36560648f43",
   "metadata": {},
   "source": [
    "[Details]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef32abc-c9fb-444e-9156-05bb5f9e1435",
   "metadata": {},
   "source": [
    "#### Residual connections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd755d2-bfbf-4af9-8852-5b5937560135",
   "metadata": {},
   "source": [
    "[Details]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bf309b-a584-43c1-8ce7-49262dc145ee",
   "metadata": {},
   "source": [
    "#### Layernorms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ecd508-8acb-45e8-8d8b-9a65b9ebf3b3",
   "metadata": {},
   "source": [
    "[Details]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d7d21b-cf7e-4ce7-ad6b-2b4e7d2f666c",
   "metadata": {},
   "source": [
    "#### Scaling up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1defac21-b22f-40d7-9cc0-b534f6360e50",
   "metadata": {},
   "source": [
    "[Details]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c338caa-0589-49a0-9ace-b179fa6acb88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-05T07:25:07.934342Z",
     "iopub.status.busy": "2023-02-05T07:25:07.934053Z",
     "iopub.status.idle": "2023-02-05T07:25:07.940165Z",
     "shell.execute_reply": "2023-02-05T07:25:07.938862Z",
     "shell.execute_reply.started": "2023-02-05T07:25:07.934322Z"
    }
   },
   "source": [
    "## [Notes on Transformer]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b675729-095d-40c2-9ba9-43f5794c17f3",
   "metadata": {},
   "source": [
    "#### Encoder vs. decoder vs. both"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3be788-107e-4786-8cde-1f9acef2007a",
   "metadata": {},
   "source": [
    "#### Batched multi-headed self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f59f7d-55d5-4525-9e94-5f77e37db10b",
   "metadata": {},
   "source": [
    "#### ChatGPT, GPT-3, pretraining vs. finetuning, RLHF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea89cde2-ec75-4b03-8ea5-2542d4c8427d",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99032892-1126-456b-8baf-424b0b00552a",
   "metadata": {},
   "source": [
    "Vaswani, A., et al. (2017).  Attention Is All You Need. arXiv:1706.03762"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efee8c82-06f7-4869-8671-eb941167b3b4",
   "metadata": {},
   "source": [
    "Brown, T.B., et al. (2020). Language Models are Few-Shot Learners. arXiv:2005.14165"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad7cab4-1542-4340-80ad-1989bcf689a2",
   "metadata": {},
   "source": [
    "__[Kaparthy's nanoGPT GitHub repo](https://github.com/karpathy/nanoGPT)__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f481fea7-f09f-409f-99fd-67c29b6a788c",
   "metadata": {},
   "source": [
    "__[Kaparthy's Youtube video](https://www.youtube.com/watch?v=kCc8FmEb1nY)__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7673317d-5bc1-4817-b349-1900a9410cc2",
   "metadata": {},
   "source": [
    "__[GitHub repo for Kaparthy's video](https://github.com/karpathy/ng-video-lecture)__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc668cf-bc7f-4d43-a6de-690a28cb9897",
   "metadata": {},
   "source": [
    "__[Google colab for Kaparthy's video](https://colab.research.google.com/drive/1JMLa53HDuA-i7ZBmqV7ZnA3c_fvtXnx-?usp=sharing)__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04df5bcd-28f1-4d99-b7f8-f6685edc947c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-09T04:06:52.202330Z",
     "iopub.status.busy": "2023-02-09T04:06:52.202069Z",
     "iopub.status.idle": "2023-02-09T04:06:52.355032Z",
     "shell.execute_reply": "2023-02-09T04:06:52.354010Z",
     "shell.execute_reply.started": "2023-02-09T04:06:52.202312Z"
    }
   },
   "source": [
    "***\n",
    "END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
