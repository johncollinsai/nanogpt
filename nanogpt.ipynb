{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75874666-e3ac-4cc7-ac24-b38a97ef5f8a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-05T21:36:41.372727Z",
     "iopub.status.busy": "2023-02-05T21:36:41.372173Z",
     "iopub.status.idle": "2023-02-05T21:36:41.379077Z",
     "shell.execute_reply": "2023-02-05T21:36:41.377483Z",
     "shell.execute_reply.started": "2023-02-05T21:36:41.372688Z"
    }
   },
   "source": [
    "# This post in a glance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "386c1731-7a29-43a4-8baf-71c9db62a787",
   "metadata": {},
   "source": [
    "Large Language Models (LLMs) have created enormous interest. Here I build and train a Generatively Pretrained Transformer (GPT) following __[Kaparthy's approah](https://www.youtube.com/watch?v=kCc8FmEb1nY)__ in order to understand the under-the-hood components of what makes a GPT work and obtain insights into how GPTs can be applied in real-life commercial applications. <br>\n",
    "\n",
    "This post is a hands on exploration of the transformer architecture first set out in the seminal 2017 AI paper __[Attention Is All You Need](https://arxiv.org/abs/1706.03762)__, arguably one of the most important AI papers written, and the 2020 GPT-3 paper __[Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)__.\n",
    "\n",
    "When we move from neural networks to transformers specifically, attention is the super-important thing.  So what is attention, exactly?  [WRITE THIS EQUATION IN LATEX]\n",
    "\n",
    "<img src=\"attention-exactly.png\" alt=\"Attention\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fed8f79-c4d8-47e5-b04f-d60e0bb25101",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "550c7378-6a41-49ff-880b-a7892a39f5cd",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Insights and takeaways:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bf034c08-7827-4e5d-a2b8-39b4ed78012f",
   "metadata": {},
   "source": [
    "> * A GPT is a large language model.  Essentially, it streams a set of predictions that are the most likely tokens (words) in response to your question, which is an input to the language model in tokens (or words).  There is nothing magical about a GPT, nor any other LLM; in response to your input they provide an optimized prediction or outcome. <br>\n",
    "> * Any magic that might exist is the magic bestowed by \"attention\". <br>\n",
    "> * So what is \"attention\", exactly? <br>\n",
    "> * Insights <br>\n",
    "> * Insights <br>\n",
    "> * Insights <br>\n",
    "> * Insights <br>\n",
    "> * Insights <br>\n",
    "> * [See Karpathy's conclusions (video 01:54:32) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af203336-ddb2-4de8-a9cb-6b7cdb690bac",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896def4a-31c8-4aa2-805e-4ecd5231a29c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-05T21:37:21.573542Z",
     "iopub.status.busy": "2023-02-05T21:37:21.572887Z",
     "iopub.status.idle": "2023-02-05T21:37:21.577911Z",
     "shell.execute_reply": "2023-02-05T21:37:21.576606Z",
     "shell.execute_reply.started": "2023-02-05T21:37:21.573523Z"
    }
   },
   "source": [
    "# Building and training a GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b9b1fc-b610-44c1-885d-1935aee8207e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Baseline (bigram) language model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "24412df2-d41e-4ff2-80dc-90cf56d28f01",
   "metadata": {
    "tags": []
   },
   "source": [
    "[INTRODUCTION]\n",
    "\n",
    "I begin with an empty file and define a transformer piece by piece.  Then I train it on a text dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bfd5cb",
   "metadata": {},
   "source": [
    "#### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "416923fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataseet in characters:  1115394\n"
     ]
    }
   ],
   "source": [
    "# get data\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(\"length of dataseet in characters: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81a44601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "# show unique characters appearing in the dataset (note the space character, which is first in the set): i.e., the vocabulary of possible characters the model can see or emit\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "95c21805",
   "metadata": {},
   "source": [
    "#### Tokenize\n",
    "\n",
    "> Build a simple encoder and  decoder: i.e., take a string, output a list of integers, where each character is a token. The approach below is similar to, but much more simplified than: __[goolge sentencepiece](https://github.com/google/sentencepiece)__ (which uses sub-word encodings) and __[OpenAI tiktoken](https://github.com/openai/tiktoken)__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17bc426f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[58, 53, 42, 39, 63, 1, 47, 57, 1, 44, 56, 47, 42, 39, 63, 6, 1, 50, 53, 53, 49, 47, 52, 45, 1, 44, 53, 56, 61, 39, 56, 42, 1, 58, 53, 1, 58, 46, 43, 1, 61, 43, 43, 49, 43, 52, 42, 2]\n",
      "today is friday, looking forward to the weekend!\n"
     ]
    }
   ],
   "source": [
    "# convert the raw text as a string into some sequence of integers according to some vocabulary of possible elements\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "# build a simple encoder and decoder, effectively a tokenizer and detokenizer\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "print(encode(\"today is friday, looking forward to the weekend!\"))\n",
    "print(decode(encode(\"today is friday, looking forward to the weekend!\")))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dc914948",
   "metadata": {},
   "source": [
    "> now I have a tokenizer and detokenizer, I can convert the raw text into a sequence of integers, i.e., I can tokenize the entire training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2edc8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
     ]
    }
   ],
   "source": [
    "# encode training dataset and store it in a torch.tensor\n",
    "import torch\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c31f70-00ec-4a57-b6b9-c610454e7f39",
   "metadata": {},
   "source": [
    "#### Train/val split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff404893-dff5-4e84-9e12-92f11cfc34b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 90:10 train:val split\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8320008e-4209-4004-b6ed-6c2cc9b3eb4c",
   "metadata": {},
   "source": [
    "#### Loading the data\n",
    "> I set the time dimension (i.e., the contexts) of the tensors feeding into the transformer equal to a maximum of 8 characters (i.e., I set block_size = 8).  Note: I train on block_size+1 because the transformer trains on the first 8 characters and predicts the +1th or 9th character.  Put another way, the transformer sees contexts from one character thru block_size. <br>\n",
    "\n",
    "> And I set the batch dimension of the tensors feeding into the transformer to 4, so batch_size = 4 (i.e., 4 independent sequences will be processed in parallel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26380bac-baa0-47a7-b19a-536b6b0f1caf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set block_size = 8 to train on []:block_size+1] = 8+1 characters at a time\n",
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ebfdae44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Illustrating how the transformer trains on the first 8 characters and predicts the +1th or 9th character:\n",
      "when input is tensor([18]), the target is 47\n",
      "when input is tensor([18, 47]), the target is 56\n",
      "when input is tensor([18, 47, 56]), the target is 57\n",
      "when input is tensor([18, 47, 56, 57]), the target is 58\n",
      "when input is tensor([18, 47, 56, 57, 58]), the target is 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]), the target is 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]), the target is 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]), the target is 58\n"
     ]
    }
   ],
   "source": [
    "# +1 because we want to predict the next character, thus block_size+1 allows us to do that, i.e., the transformer trains on the first 8 characters and predicts the +1th or 9th character\n",
    "# to illustrate:\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "print('Illustrating how the transformer trains on the first 8 characters and predicts the +1th or 9th character:')\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f'when input is {context}, the target is {target}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4ca53410",
   "metadata": {},
   "source": [
    "#### A quick note on random seed selection\n",
    "In an interesting __[paper](https://arxiv.org/abs/2109.08203)__ David Picard investigates the effect of random seed selection on accuracy when using deep learning architectures for computer vision and posits that Torch.manual_seed(3407) is all you need!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1fb66009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the tensor input to the transformer: \n",
      " tensor([[32, 39, 49, 43,  1, 58, 46, 53],\n",
      "        [59, 56,  1, 54, 56, 47, 52, 41],\n",
      "        [57, 53, 51, 43,  1, 51, 43, 56],\n",
      "        [57,  1, 58, 56, 59, 43,  2,  0]])\n"
     ]
    }
   ],
   "source": [
    "# I set the batch dimension of the tensors feeding into the transformer to 4, so batch_size = 4 (i.e., 4 independent sequences will be processed in parallel).\n",
    "torch.manual_seed(3407)\n",
    "batch_size = 4\n",
    "block_size = 8  \n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,)) \n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "\n",
    "print('Here is the tensor input to the transformer:',\n",
    "      '\\n', \n",
    "      xb      \n",
    "      )  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "625a9b6b-3710-4454-a648-2daef588b99f",
   "metadata": {},
   "source": [
    "#### baseline (bigram) language model\n",
    "\n",
    "Following __[Karpathy](https://www.youtube.com/watch?v=kCc8FmEb1nY)__, I implement a very simple neural network, the bigram language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6bec0de5-afc3-4ada-8c24-ef2e5325d7e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits shape: torch.Size([32, 65])\n",
      "loss: tensor(4.4231, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "T&jqF$cy$c'WsTh3k!eloQJWlLacKbtbj.\n",
      "JW!wwU&OBm;R;PrHwwe!!NMiWsyVoHRqDr\n",
      ";\n",
      "c3OGIUstnsscP- Fwzq.klOnMX'3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(3407)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx and targets are both (B, T) tensors of integers\n",
    "        logits = self.token_embedding_table(idx) # (B, T, C, i.e., a batch by time (context) by channel tensor, where channel is vocab size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # reorganize logits tensor from (B, T, C) to (B*T, C) in order to fit pytorch's cross_entropy loss function\n",
    "            B, T, C = logits.shape \n",
    "            logits = logits.view(B*T, C) \n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets) # cross_entropy here computes negative log likelihood loss\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is a (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get the probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat([idx, idx_next], dim=1) # (B, T+1)\n",
    "\n",
    "        return idx\n",
    "    \n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print('logits shape:', logits.shape)\n",
    "print('loss:', loss)\n",
    "\n",
    "print(decode(m.generate(idx = torch.zeros((1,1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f2830757",
   "metadata": {},
   "source": [
    "> The model's predictions show language-like structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a3b698-2682-4dfc-9ff1-8a420d51e037",
   "metadata": {},
   "source": [
    "#### Training the bigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c539ac77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pytorch optimizer\n",
    "optimizer = torch.optim.Adam(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "df0b42ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.4564297199249268\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32 # increase the batch size from 4 to 32 to speed up training\n",
    "for steps in range(1000): # increase the number of steps to train for, to improve results\n",
    "\n",
    "    # get a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print('loss:', loss.item()) # training for 1000 steps brings the loss down to ~2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2c16952a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SubwKHhAn:lHER3xJgNndT.,q3EUyafRq'iSoPqDp,E:b&JJj&&bpuc3O'vx!'kHkQN\n",
      "3AY\n",
      "?dxd3Zj;I LppuDjQAV,ti!N?d3J?? pI NosDHwasaRhEm?KQjuucwVmpVArXGV OsNpYpL'YnuoQDPUxIwTAoQvGE$:JK wALafiRTvxUJG'IOkd3cHf.iCiUVo$:ax&\n",
      "Vnsf!of3m!stYoQfdKAFwS;DD&d&WJFJF3LujmeOeost pa?qCjhLMVj&'RD,pBpbpKerzHYzw\n",
      "ThXcnMPUiwwSo'ya?rMXNOO?q'ffdyklqe!PUAd!,.ngIE zb3raig?ngJTwsK\n",
      "shhLNR,GH3RXcbyRwV,I'UlTg$3lOZn GW'I:JFoQF3pk&qEbZti!NJpIXs?YwSt&XR,Whd3kmheVA;,-. k&W:ENIn3Ar!N$ULgpuv GW'iwlZu-:.xRD,iWVyW.M:bGHIXULRgKXZtFwcK d:emzzMlvIk;DQ\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx = torch.zeros((1,1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "12526d05",
   "metadata": {},
   "source": [
    "> The model's predictions show a somewhat better language-like structure "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a589398b-7567-4c24-91b0-5c32d255be16",
   "metadata": {},
   "source": [
    "[Details]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a213751c-62eb-4172-8ead-64dfb75779b2",
   "metadata": {},
   "source": [
    "#### Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34484a3-ecf5-4381-8f52-0d7b7fee0498",
   "metadata": {},
   "source": [
    "[Details]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d1a718-bb5f-4479-a576-465adde0082c",
   "metadata": {},
   "source": [
    "## Self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65089f35-33e4-45de-821c-e56ac1f5863e",
   "metadata": {},
   "source": [
    "#### Averaging past context with for loops (weakest form of aggregation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb497036-c809-44fc-8152-331cb6594706",
   "metadata": {},
   "source": [
    "[Details]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8b09bd-e7b5-4de1-985f-8423193eb8ac",
   "metadata": {},
   "source": [
    "#### Self-attention: matrix multiply as weighted aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305043fc-1380-4b25-94ee-a16fe0950ee7",
   "metadata": {},
   "source": [
    "[Details]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0b3945-9457-4b21-9107-800c43e3486b",
   "metadata": {},
   "source": [
    "#### Using matrix multiply"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ba9f42-1d4b-4aba-bc74-8d8a58f69272",
   "metadata": {},
   "source": [
    "[Details]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b513a4-5e38-46b7-9d4c-eb7fd43ce2d9",
   "metadata": {},
   "source": [
    "#### Adding softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8783fa4f-c174-4a88-80cb-be8738e08ae1",
   "metadata": {},
   "source": [
    "[Details]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f52a3b2-cff8-484f-bc62-4489d7b678cc",
   "metadata": {},
   "source": [
    "#### Positional encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169bd6e6-1548-405e-8aaa-a7d8658e6423",
   "metadata": {},
   "source": [
    "[Details]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8a01da-694c-4159-85ca-b49135fd23ef",
   "metadata": {},
   "source": [
    "#### Self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f54128-e5c2-467e-bd81-2a40327dc60a",
   "metadata": {},
   "source": [
    "[Details]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc35e5cc-a02e-40da-8f57-ee09ffa4c267",
   "metadata": {},
   "source": [
    "Notes\n",
    "\n",
    "> * Attention as communication <br>\n",
    "\n",
    "> * Attention has no notion of space, operates over sets\n",
    "\n",
    "> * There is no communication across batch dimension\n",
    "\n",
    "> * Encoder blocks vs. decoder blocks\n",
    "\n",
    "> * Attention vs. self-attention vs. cross-attention\n",
    "\n",
    "> * \"Scaled\" self-attention.  Why divide by sqrt(head_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f818f9-4516-4c3f-9b8f-2d8de4beb9b1",
   "metadata": {},
   "source": [
    "## Building the Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f07d79-0203-4558-93a0-5e8974b317ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-09T04:03:37.171153Z",
     "iopub.status.busy": "2023-02-09T04:03:37.170812Z",
     "iopub.status.idle": "2023-02-09T04:03:37.176160Z",
     "shell.execute_reply": "2023-02-09T04:03:37.174873Z",
     "shell.execute_reply.started": "2023-02-09T04:03:37.171132Z"
    }
   },
   "source": [
    "#### Inserting a single self-attention block "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6081db1b-3530-4863-aff1-ee0d33f7f24f",
   "metadata": {},
   "source": [
    "[Details]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4111fee3-add9-4c60-926d-b02395b6666a",
   "metadata": {},
   "source": [
    "#### Multi-headed self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e56e79-4c09-470e-bf40-b6afc39ff475",
   "metadata": {},
   "source": [
    "[Details]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6addebf6-d82d-49b8-b803-8b4a1490459a",
   "metadata": {},
   "source": [
    "#### feedforward layers of transformer block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f920a6-8c7b-454c-b586-e36560648f43",
   "metadata": {},
   "source": [
    "[Details]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef32abc-c9fb-444e-9156-05bb5f9e1435",
   "metadata": {},
   "source": [
    "#### Residual connections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd755d2-bfbf-4af9-8852-5b5937560135",
   "metadata": {},
   "source": [
    "[Details]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bf309b-a584-43c1-8ce7-49262dc145ee",
   "metadata": {},
   "source": [
    "#### Layernorms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ecd508-8acb-45e8-8d8b-9a65b9ebf3b3",
   "metadata": {},
   "source": [
    "[Details]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d7d21b-cf7e-4ce7-ad6b-2b4e7d2f666c",
   "metadata": {},
   "source": [
    "#### Scaling up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1defac21-b22f-40d7-9cc0-b534f6360e50",
   "metadata": {},
   "source": [
    "[Details]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c338caa-0589-49a0-9ace-b179fa6acb88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-05T07:25:07.934342Z",
     "iopub.status.busy": "2023-02-05T07:25:07.934053Z",
     "iopub.status.idle": "2023-02-05T07:25:07.940165Z",
     "shell.execute_reply": "2023-02-05T07:25:07.938862Z",
     "shell.execute_reply.started": "2023-02-05T07:25:07.934322Z"
    }
   },
   "source": [
    "## [Notes on Transformer]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b675729-095d-40c2-9ba9-43f5794c17f3",
   "metadata": {},
   "source": [
    "#### Encoder vs. decoder vs. both"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3be788-107e-4786-8cde-1f9acef2007a",
   "metadata": {},
   "source": [
    "#### Batched multi-headed self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f59f7d-55d5-4525-9e94-5f77e37db10b",
   "metadata": {},
   "source": [
    "#### ChatGPT, GPT-3, pretraining vs. finetuning, RLHF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea89cde2-ec75-4b03-8ea5-2542d4c8427d",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efee8c82-06f7-4869-8671-eb941167b3b4",
   "metadata": {},
   "source": [
    "Brown, T.B., et al. (2020). Language Models are Few-Shot Learners. arXiv:2005.14165"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7333d09d",
   "metadata": {},
   "source": [
    "Picard, D. (2021). Torch.manual_seed(3407) is all you need: On the influence of random seeds in deep learning architectures for computer vision. arXiv:2109.08203 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99032892-1126-456b-8baf-424b0b00552a",
   "metadata": {},
   "source": [
    "Vaswani, A., et al. (2017).  Attention Is All You Need. arXiv:1706.03762"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad7cab4-1542-4340-80ad-1989bcf689a2",
   "metadata": {},
   "source": [
    "__[Kaparthy's nanoGPT GitHub repo](https://github.com/karpathy/nanoGPT)__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f481fea7-f09f-409f-99fd-67c29b6a788c",
   "metadata": {},
   "source": [
    "__[Kaparthy's Youtube video](https://www.youtube.com/watch?v=kCc8FmEb1nY)__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7673317d-5bc1-4817-b349-1900a9410cc2",
   "metadata": {},
   "source": [
    "__[GitHub repo for Kaparthy's video](https://github.com/karpathy/ng-video-lecture)__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc668cf-bc7f-4d43-a6de-690a28cb9897",
   "metadata": {},
   "source": [
    "__[Google colab for Kaparthy's video](https://colab.research.google.com/drive/1JMLa53HDuA-i7ZBmqV7ZnA3c_fvtXnx-?usp=sharing)__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04df5bcd-28f1-4d99-b7f8-f6685edc947c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-09T04:06:52.202330Z",
     "iopub.status.busy": "2023-02-09T04:06:52.202069Z",
     "iopub.status.idle": "2023-02-09T04:06:52.355032Z",
     "shell.execute_reply": "2023-02-09T04:06:52.354010Z",
     "shell.execute_reply.started": "2023-02-09T04:06:52.202312Z"
    }
   },
   "source": [
    "***\n",
    "END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
